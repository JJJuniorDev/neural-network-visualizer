import { Component, Input } from '@angular/core';
import { CommonModule } from '@angular/common';
import { LayerConfig } from '../../models/layer-config.model';

@Component({
  selector: 'app-process-view',
  imports: [CommonModule],
  templateUrl: './process-view.html',
  standalone: true,
  styleUrls: ['./process-view.css']
})
export class ProcessView {
  @Input() config!: LayerConfig;

  // Colori per i parametri
  getWeightColor(): string {
    if (this.config.weight > 0) return '#06d6a0';
    if (this.config.weight < 0) return '#ef476f';
    return '#6c757d';
  }

  getBiasColor(): string {
    if (this.config.bias > 0) return '#4361ee';
    if (this.config.bias < 0) return '#f72585';
    return '#6c757d';
  }

  getActivationColor(): string {
    const colors: {[key: string]: string} = {
      'linear': '#6c757d',
      'sigmoid': '#f72585',
      'relu': '#06d6a0',
      'tanh': '#4361ee',
      'leaky_relu': '#3a0ca3',
      'prelu': '#7209b7',
      'elu': '#b5179e',
      'selu': '#f72585',
      'swish': '#4cc9f0',
      'mish': '#f8961e',
      'gelu': '#f94144',
      'softplus': '#577590',
      'softmax': '#f9844a'
    };
    return colors[this.config.activation] || '#6c757d';
  }

  getActivationColorLight(): string {
    const colors: {[key: string]: string} = {
      'linear': '#494c50',
      'sigmoid': '#d60e72',
      'relu': '#169e4a',
      'tanh': '#005a9a',
      'leaky_relu': '#2a0570',
      'prelu': '#52058a',
      'elu': '#850a70',
      'selu': '#d60e72',
      'swish': '#1f8fa5',
      'mish': '#b8600a',
      'gelu': '#b02124',
      'softplus': '#2d455a',
      'softmax': '#b05a2a'
    };
    return colors[this.config.activation] || '#f8f9fa';
  }

  getActivationIcon(): string {
    const icons: {[key: string]: string} = {
      'linear': 'ðŸ“',
      'sigmoid': 'S',
      'relu': 'R',
      'tanh': 'T',
      'leaky_relu': 'L',
      'prelu': 'P',
      'elu': 'E',
      'selu': 'âš¡',
      'swish': 'ðŸ',
      'mish': 'ã€½ï¸',
      'gelu': 'ðŸ§ ',
      'softplus': 'ðŸ«§',
      'softmax': 'ðŸŽ¯'
    };
    return icons[this.config.activation] || 'âš¡';
  }

  getActivationName(): string {
    const names: {[key: string]: string} = {
      'linear': 'Linear Activation',
      'sigmoid': 'Sigmoid Activation',
      'relu': 'ReLU Activation',
      'tanh': 'Tanh Activation',
      'leaky_relu': 'Leaky ReLU Activation',
      'prelu': 'PReLU Activation',
      'elu': 'ELU Activation',
      'selu': 'SELU Activation',
      'swish': 'Swish Activation',
      'mish': 'Mish Activation',
      'gelu': 'GELU Activation',
      'softplus': 'Softplus Activation',
      'softmax': 'Softmax Activation'
    };
    return names[this.config.activation] || this.config.activation;
  }

  getActivationFormula(): string {
    switch(this.config.activation) {
      case 'linear': return 'f(x) = x';
      case 'sigmoid': return 'f(x) = 1/(1+eâ»Ë£)';
      case 'relu': return 'f(x) = max(0, x)';
      case 'tanh': return 'f(x) = tanh(x)';
      case 'leaky_relu': return 'f(x) = max(0.01x, x)';
      case 'prelu': return 'f(x) = max(ax, x) con a appreso';
      case 'elu': return 'f(x) = x (x>0), Î±(eË£-1) (xâ‰¤0)';
      case 'selu': return 'f(x) = Î»Â·ELU(x, Î±) con Î»â‰ˆ1.05, Î±â‰ˆ1.67';
      case 'swish': return 'f(x) = x Â· sigmoid(x)';
      case 'mish': return 'f(x) = x Â· tanh(ln(1+eË£))';
      case 'gelu': return 'f(x) = x Â· Î¦(x)';
      case 'softplus': return 'f(x) = ln(1+eË£)';
      case 'softmax': return 'Ïƒ(záµ¢) = eá¶»â±/âˆ‘â±¼eá¶»Ê²';
      default: return '';
    }
  }

  getActivationExplanation(): string {
    switch(this.config.activation) {
      case 'linear':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
La funzione lineare Ã¨ come un copia e incolla: qualunque valore entra, lo stesso valore esce. 
CiÃ² che vedi da un lato Ã¨ identico dall'altro. Il peso (w) decide quanto 
inclinare la linea, il bias (b) decide dove la linea incrocia l'asse verticale.

ðŸ“ **EXTRA**:
Questo tipo di attivazione Ã¨ fondamentale nei layer di output per problemi di regressione, dove si richiedono valori reali non limitati. 
Tuttavia, l'assenza di non-linearitÃ  limita la capacitÃ  della rete di apprendere relazioni complesse, 
rendendo una rete con solo attivazioni lineari equivalente a un modello lineare, indipendentemente 
dal numero di layer (composizione di funzioni lineari = funzione lineare).`;
      
      case 'sigmoid':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
La sigmoide Ã¨ come un compressore che prende qualsiasi numero e lo schiaccia tra 0 e 1. 
Numeri grandi positivi diventano quasi 1, numeri grandi negativi diventano quasi 0. 
Ãˆ perfetta quando vuoi esprimere una probabilitÃ : "quanto sono sicuro che questo sia un gatto?" 
(0 = non lo Ã¨, 1 = lo Ã¨ sicuramente). La curva a S trasforma valori lineari in decisioni graduali.

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
La funzione sigmoide mappa l'intero asse reale nell'intervallo (0, 1), e  puÃ² causare il problema del vanishing gradient nelle reti profonde. 
Storicamente utilizzata nei perceptron e nei layer di output per classificazione binaria, la sigmoide 
fornisce output interpretabili come probabilitÃ  posteriori P(y=1|x). Tuttavia, l'output non centrato a zero 
puÃ² causare oscillazioni durante la discesa del gradiente e la saturazione dei neuroni.`;
      
      case 'relu':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
ReLU Ã¨ come un "butta via i negativi": se il numero Ã¨ positivo, lo lascia passare tale e quale; 
se Ã¨ negativo, lo trasforma in zero. Immagina un rubinetto che lascia scorrere l'acqua solo in una direzione. 
Ãˆ semplice, veloce e permette alla rete di concentrarsi solo su ciÃ² che Ã¨ "attivo". I neuroni negativi 
vengono spenti, creando una rappresentazione "sparsa" (pochi neuroni attivi contemporaneamente).

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
La Rectified Linear Unit (ReLU) introduce non-linearitÃ  mantenendo il gradiente costante (1) per z > 0, mitigando il problema del vanishing gradient rispetto alle funzioni 
saturanti. La sua derivata Ã¨ f'(z) = 1 per z > 0, 0 per z < 0.
 ReLU produce rappresentazioni sparse (attivazione di â‰ˆ50% dei neuroni), 
migliorando l'efficienza computazionale. Tuttavia, soffre del problema "dying ReLU" quando i gradienti 
negativi azzerano permanentemente i neuroni. Varianti come LeakyReLU mitigano questo problema permettendo gradienti negativi non nulli.`;
      
      case 'tanh':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
Tanh Ã¨ come una sigmoide "bilanciata": invece di schiacciare tra 0 e 1, schiaccia tra -1 e 1. 
Numeri grandi positivi diventano +1, numeri grandi negativi diventano -1, e zero rimane zero. 
Ãˆ utile quando vuoi rappresentare valori che possono essere sia positivi che negativi, 
come le differenze o i bilanciamenti. La curva Ã¨ centrata a zero, il che aiuta l'apprendimento.

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
La tangente iperbolica Ã¨ una funzione dispari che mappa l'input nell'intervallo (-1, 1), con tanh(0) = 0. 
Rispetto alla sigmoide, tanh Ã¨ centrata a zero (output medio zero), proprietÃ  che favorisce la convergenza durante la backpropagation riducendo lo shift dei gradienti. 
Tanh combina la non-linearitÃ  con output simmetrici, rendendola preferibile alla sigmoide nei layer nascosti quando i dati hanno media zero. 
Tuttavia, soffre anch'essa di saturazione per |z| grandi, con conseguente vanishing gradient nelle reti profonde.`;

      case 'leaky_relu':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
Leaky ReLU Ã¨ come ReLU ma con un "rubinetto che perde": invece di azzerare completamente i valori negativi, 
lascia passare una piccola quantitÃ  (1%). I neuroni negativi non muoiono mai completamente, 
mantenendo sempre un piccolo flusso di informazioni.

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
Leaky ReLU introduce una pendenza piccola ma non nulla per x<0, garantendo gradienti non nulli anche per input negativi.
Risolve il problema "dying ReLU" mantenendo l'efficienza computazionale. Il parametro Î± Ã¨ tipicamente fissato a 0.01, 
ma puÃ² essere ottimizzato.`;
      
      case 'prelu':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
PReLU Ã¨ come Leaky ReLU ma con un superpotere: impara da sola qual Ã¨ il miglior valore per la pendenza negativa 
durante l'addestramento. Ãˆ come se il neurone potesse regolare la sua "perdita" in base ai dati.

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
Parametric ReLU (PReLU) generalizza Leaky ReLU rendendo il coefficiente Î± un parametro apprendibile 
durante la backpropagation. Questo permette a ogni neurone di adattare la sua risposta ai valori negativi 
in modo ottimale per il task specifico, aumentando la flessibilitÃ  del modello.`;
      
      case 'elu':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
ELU Ã¨ come ReLU ma con una transizione morbida per i valori negativi: invece di uno scalino netto, 
c'Ã¨ una curva liscia che tende gradualmente a -Î±. Questo rende l'apprendimento piÃ¹ stabile.

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
Exponential Linear Unit (ELU). La transizione liscia in x=0 e la saturazione verso -Î± per xâ†’ -âˆž producono output medi piÃ¹ vicini a zero, accelerando la convergenza 
e riducendo il bias shift.`;
      
      case 'selu':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
SELU Ã¨ una versione "magica" di ELU che mantiene automaticamente i dati bilanciati: dopo ogni layer, 
la media dei valori resta vicina a 0 e la varianza vicina a 1. Non serve la normalizzazione!

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
Scaled ELU (SELU) con Î»â‰ˆ1.0507 e Î±â‰ˆ1.67326 possiede la proprietÃ  di auto-normalizzazione: 
per una rete profonda, la media e varianza degli output convergono a 0 e 1 rispettivamente, 
eliminando la necessitÃ  di batch normalization.`;
      
      case 'swish':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
Swish Ã¨ come una ReLU "morbida" con una gobba: per valori negativi piccoli, invece di essere zero, 
ha un leggero "rimbalzo" negativo prima di salire. Questo piccolo trucco aiuta l'apprendimento.

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
Swish (f(x) = xÂ·sigmoid(x)) Ã¨ una funzione liscia, non monotona, che permette un miglior flusso del gradiente. 
Studi dimostrano che supera ReLU in reti profonde (>40 layer) grazie alla sua capacitÃ  di preservare 
piccoli gradienti negativi.`;
      
      case 'mish':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
Mish Ã¨ come Swish ma ancora piÃ¹ raffinato: crea una curva piÃ¹ dolce e continua, 
permettendo alle informazioni di fluire meglio attraverso la rete.

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
Mish Ã¨ auto-regularizzante e mantiene piccole quantitÃ  di informazioni negative, 
migliorando la propagazione del gradiente. In benchmark recenti, Mish ha superato Swish e ReLU 
in diverse architetture profonde.`;
      
      case 'gelu':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
GELU decide se attivare un neurone in base a "quanto Ã¨ probabilmente positivo". 
Ãˆ come un semaforo intelligente che valuta le probabilitÃ  prima di decidere.

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
GELU (Gaussian Error Linear Unit) pesa l'input per la sua probabilitÃ  di essere positivo 
secondo la distribuzione gaussiana. Ãˆ la funzione standard nei Transformer (BERT, GPT) 
perchÃ© fornisce una transizione piÃ¹ naturale tra attivazione e non-attivazione.`;
      
      case 'softplus':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
Softplus Ã¨ come una ReLU "morbida": invece di uno scalino netto a zero, la curva si piega dolcemente, 
non diventando mai completamente zero.

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
Softplus Ã¨ un'approssimazione liscia e differenziabile di ReLU. 
Mantiene la proprietÃ  di essere sempre positiva ma con gradienti definiti ovunque, 
utile in contesti dove serve differenziabilitÃ  stretta.`;
      
      case 'softmax':
        return `ðŸ“š **SPIEGAZIONE DUMMY**:
Softmax trasforma un vettore di numeri in probabilitÃ  che sommano a 1. Vince il piÃ¹ grande, 
ma gli altri hanno comunque la loro "piccola chance". Ãˆ come una gara dove tutti ricevono 
una fetta della torta, ma il vincitore prende la fetta piÃ¹ grande.

ðŸ“ **SPIEGAZIONE ACCADEMICA**:
Softmax normalizza un vettore in distribuzione di probabilitÃ , enfatizzando il massimo. 
Ãˆ l'attivazione standard per il layer di output in problemi di classificazione multi-classe, 
producendo output interpretabili come probabilitÃ  P(y=i|x). La funzione esponenziale enfatizza 
le differenze tra i valori, rendendo il massimo piÃ¹ pronunciato.`;
      
      default:
        return '';
    }
  }

  getActivationTags(): string[] {
    switch(this.config.activation) {
      case 'linear':
        return [
          'ðŸ“ˆ Regressione',
          'âš–ï¸ Proporzionale',
          'ðŸ”· Output layer',
          'ðŸ“‰ Nessuna non-linearitÃ ',
          'ðŸ§® w = pendenza, b = intercetta'
        ];
      
      case 'sigmoid':
        return [
          'ðŸŽ² ProbabilitÃ  [0,1]',
          'âšª Classificazione binaria',
          'ðŸ“‰ Vanishing gradient',
          'ðŸ”„ Ïƒ(-z) = 1-Ïƒ(z)',
          'ðŸ§  Neuroni saturi per |z| > 4',
          'ðŸŽ¯ Output interpretabile'
        ];
      
      case 'relu':
        return [
          'âš¡ Efficienza computazionale',
          'ðŸ§  Reti profonde',
          '0ï¸âƒ£ SparsitÃ  (50% neuroni spenti)',
          'ðŸ“ˆ Gradiente costante per z>0',
          'ðŸ’€ Dying ReLU per z<0',
          'ðŸš€ Non saturazione'
        ];
      
      case 'tanh':
        return [
          'ðŸ”„ Centrato a zero',
          'ðŸ“Š Gradienti forti',
          'âž• Valori negativi/positivi',
          'ðŸ“‰ Saturazione a Â±1',
          'âš–ï¸ Output simmetrico',
          'ðŸ§  Preferita a sigmoide in hidden layer'
        ];

      case 'leaky_relu':
        return [
          'ðŸ’§ Evita neuroni morti',
          'ðŸ“‰ Pendenza 0.01 per x<0',
          'âš¡ Efficiente come ReLU',
          'ðŸ”„ Gradiente sempre presente',
          'ðŸ§  Î±=0.01 (tipico)'
        ];
      
      case 'prelu':
        return [
          'ðŸ“š Parametro apprendibile',
          'ðŸ”„ Adattivo per ogni neurone',
          'âš¡ Massima flessibilitÃ ',
          'ðŸ§  Î± ottimizzato in training'
        ];
      
      case 'elu':
        return [
          'ðŸ“‰ Transizione liscia',
          'ðŸŽ¯ Output medio â‰ˆ 0',
          'ðŸ§  Convergenza veloce',
          'âš¡ Î± tipico = 1.0',
          'ðŸ”„ Saturazione negativa'
        ];
      
      case 'selu':
        return [
          'âœ¨ Auto-normalizzante',
          'ðŸ“Š Media=0, Varianza=1',
          'ðŸ§  Nessuna batch norm',
          'âš¡ Î»â‰ˆ1.05, Î±â‰ˆ1.67',
          'ðŸš€ Reti molto profonde'
        ];
      
      case 'swish':
        return [
          'ðŸ Non monotona',
          'ðŸ“ˆ Gradiente fluido',
          'ðŸ§  Superiore a ReLU in reti profonde',
          'âš¡ xÂ·sigmoid(x)',
          'ðŸš€ Usata in YOLO'
        ];
      
      case 'mish':
        return [
          'ã€½ï¸ Auto-regularizzante',
          'ðŸ“Š Superiore a Swish',
          'ðŸ§  Gradiente continuo',
          'âš¡ xÂ·tanh(softplus(x))',
          'ðŸš€ Benchmark leader'
        ];
      
      case 'gelu':
        return [
          'ðŸ§  Standard nei Transformer',
          'ðŸ“Š BERT, GPT, LLM',
          'âš¡ Ponderazione probabilistica',
          'ðŸ“ˆ Transizione naturale',
          'ðŸš€ NLP moderno'
        ];
      
      case 'softplus':
        return [
          'ðŸ«§ ReLU morbida',
          'ðŸ“ˆ Sempre differenziabile',
          'âš¡ Mai zero',
          'ðŸ§  f(x) = ln(1+eË£)'
        ];
      
      case 'softmax':
        return [
          'ðŸŽ¯ Distribuzione probabilitÃ ',
          'ðŸ“Š Somma = 1',
          'âš¡ Multi-class output',
          'ðŸ§  Enfatizza il massimo',
          'ðŸš€ Classificazione'
        ];
      
      default:
        return [];
    }
  }

  getActivationExample(): string {
    const x = 2;
    const z = this.config.weight * x + this.config.bias;
    
    switch(this.config.activation) {
      case 'linear':
        return `f(${z.toFixed(2)}) = ${z.toFixed(2)} (nessuna trasformazione)`;
      
      case 'sigmoid': {
        const result = 1 / (1 + Math.exp(-z));
        const confidence = (result * 100).toFixed(1);
        return `f(${z.toFixed(2)}) = ${result.toFixed(3)} â†’ probabilitÃ  del ${confidence}%`;
      }
      
      case 'relu': {
        const result = Math.max(0, z);
        const status = z < 0 ? 'neurone spento (0)' : `neurone attivo (${result.toFixed(2)})`;
        return `f(${z.toFixed(2)}) = ${result.toFixed(2)} â†’ ${status}`;
      }
      
      case 'tanh': {
        const result = Math.tanh(z);
        const interpretation = result > 0 ? 'positivo' : result < 0 ? 'negativo' : 'neutro';
        return `f(${z.toFixed(2)}) = ${result.toFixed(3)} â†’ valore ${interpretation}`;
      }

      case 'leaky_relu': {
        const result = z > 0 ? z : 0.01 * z;
        return `f(${z.toFixed(2)}) = ${result.toFixed(2)} (pendenza 0.01 per negativi)`;
      }
      
      case 'prelu': {
        const result = z > 0 ? z : 0.01 * z; // Semplificato
        return `f(${z.toFixed(2)}) = ${result.toFixed(2)} (Î± appreso)`;
      }
      
      case 'elu': {
        const alpha = 1.0;
        const result = z > 0 ? z : alpha * (Math.exp(z) - 1);
        return `f(${z.toFixed(2)}) = ${result.toFixed(3)} (transizione liscia)`;
      }
      
      case 'selu': {
        const alpha = 1.67326;
        const lambda = 1.0507;
        const elu = z > 0 ? z : alpha * (Math.exp(z) - 1);
        const result = lambda * elu;
        return `f(${z.toFixed(2)}) = ${result.toFixed(3)} (auto-normalizzante)`;
      }
      
      case 'swish': {
        const result = z / (1 + Math.exp(-z));
        return `f(${z.toFixed(2)}) = ${result.toFixed(3)} (xÂ·sigmoid(x))`;
      }
      
      case 'mish': {
        const softplus = Math.log(1 + Math.exp(z));
        const result = z * Math.tanh(softplus);
        return `f(${z.toFixed(2)}) = ${result.toFixed(3)} (xÂ·tanh(softplus(x)))`;
      }
      
      case 'gelu': {
        const result = 0.5 * z * (1 + Math.tanh(Math.sqrt(2/Math.PI) * (z + 0.044715 * Math.pow(z, 3))));
        return `f(${z.toFixed(2)}) = ${result.toFixed(3)} (approssimazione GELU)`;
      }
      
      case 'softplus': {
        const result = Math.log(1 + Math.exp(z));
        return `f(${z.toFixed(2)}) = ${result.toFixed(3)} (ReLU morbida)`;
      }
      
      case 'softmax': {
        const result = Math.exp(z) / (Math.exp(z) + 1);
        return `f(${z.toFixed(2)}) = ${result.toFixed(3)} (probabilitÃ  per 2 classi)`;
      }
      
      default:
        return '';
    }
  }
}